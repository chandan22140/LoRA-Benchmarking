{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"                     ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nfrom torchvision.models import vit_b_16, ViT_B_16_Weights\nfrom torch.utils.tensorboard import SummaryWriter\nimport time\nimport copy\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nfrom torchvision.models import vit_b_16, ViT_B_16_Weights\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms, datasets, models\nfrom torch.utils.data import DataLoader\nfrom torchvision.models import vit_b_16, ViT_B_16_Weights\nfrom typing import Dict\nimport math\nfrom typing import Optional, List\n\nimport time\nimport copy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\n# Import f1_score for F1 calculation\nfrom sklearn.metrics import f1_score\n\n# Hyperparameters\nEPOCHS = 25\nbatch_size = 64\nBASE_LR = 1e-3\nWEIGHT_DECAY = 0.03\nDROPOUT = 0.1\nLORA_ALPHA = 32\nLORA_DROPOUT = 0\nR_LORA_VALUES = [8]  # LoRA ranks to evaluate\n\n\nclass LoRALayer():\n    def __init__(\n        self,\n        r: int,\n        lora_alpha: int,\n        lora_dropout: float,\n        merge_weights: bool,\n    ):\n        self.r = r\n        self.lora_alpha = lora_alpha\n\n        # Optional dropout\n        if lora_dropout > 0.:\n            self.lora_dropout = nn.Dropout(p=lora_dropout)\n        else:\n            self.lora_dropout = lambda x: x\n        # Mark the weight as unmerged\n        self.merged = False\n        self.merge_weights = merge_weights\n\n\nclass xLinear(nn.Linear, LoRALayer):\n    # LoRA implemented in a dense layer\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        r: int = 0,\n        lora_alpha: int = 32,\n        lora_dropout: float = 0.0,\n        fan_in_fan_out: bool = False,\n        merge_weights: bool = True,\n        pretrained_weights=None,  # Added to accept pretrained weights\n        pretrained_bias=None,     # Added to accept pretrained bias\n        **kwargs\n    ):\n        super().__init__(in_features, out_features, **kwargs)\n        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n                           merge_weights=merge_weights)\n\n        self.fan_in_fan_out = fan_in_fan_out\n        if pretrained_weights is not None:\n            self.weight.data = pretrained_weights\n        if pretrained_bias is not None:\n            self.bias.data = pretrained_bias\n\n        # Actual trainable parameters\n        if r > 0:\n            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))\n            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))\n            self.scaling = self.lora_alpha / self.r\n            self.weight.requires_grad = False\n        self._initialize_lora_parameters()  # Only initialize LoRA parameters\n        if fan_in_fan_out:\n            self.weight.data = self.weight.data.transpose(0, 1)\n\n    def _initialize_lora_parameters(self):\n        \"\"\"\n        Initialize only the LoRA-specific parameters (lora_A and lora_B).\n        Avoid reinitializing self.weight or self.bias to preserve pretrained values.\n        \"\"\"\n        if hasattr(self, 'lora_A'):\n            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n            nn.init.zeros_(self.lora_B)\n            \n    def train(self, mode: bool = True):\n        def T(w):\n            return w.transpose(0, 1) if self.fan_in_fan_out else w\n        nn.Linear.train(self, mode)\n        if mode:\n            if self.merge_weights and self.merged:\n                # Make sure that the weights are not merged\n                if self.r > 0:\n                    self.weight.data -= T(self.lora_B @ self.lora_A) * self.scaling\n                self.merged = False\n        else:\n            if self.merge_weights and not self.merged:\n                # Merge the weights and mark it\n                if self.r > 0:\n                    self.weight.data += T(self.lora_B @ self.lora_A) * self.scaling\n                self.merged = True\n\n    def forward(self, x: torch.Tensor):\n        def T(w):\n            return w.transpose(0, 1) if self.fan_in_fan_out else w\n        if self.r > 0 and not self.merged:\n            result = F.linear(x, T(self.weight), bias=self.bias)\n            result += (self.lora_dropout(x) @ self.lora_A.transpose(0, 1) @ self.lora_B.transpose(0, 1)) * self.scaling\n            return result\n        else:\n            return F.linear(x, T(self.weight), bias=self.bias)\n\ndef replace_linear_with_lora(module: nn.Module, parent_name='', skip_substring='heads.head'):\n    \"\"\"\n    Recursively replace all nn.Linear modules with LoRALayer.Linear,\n    while preserving pretrained weights and biases and skipping specific submodules.\n    \"\"\"\n    for name, child in list(module.named_children()):\n        # Form the fully qualified name (like 'encoder.layer1.linear')\n        module_path = f\"{parent_name}.{name}\" if parent_name else name\n\n        # Recursively apply to child modules first\n        replace_linear_with_lora(child, parent_name=module_path, skip_substring=skip_substring)\n\n        if isinstance(child, nn.Linear) and skip_substring not in module_path:\n            # Extract pretrained weights and bias\n            pretrained_weights = child.weight.data.clone()\n            pretrained_bias = child.bias.data.clone() if child.bias is not None else None\n\n            # Replace the nn.Linear with LoRA-wrapped Linear\n            lora_linear = xLinear(\n                in_features=child.in_features,\n                out_features=child.out_features,\n                r=R_LORA,\n                lora_alpha=LORA_ALPHA,\n                lora_dropout=LORA_DROPOUT,\n                pretrained_weights=pretrained_weights,\n                pretrained_bias=pretrained_bias,\n            )\n            setattr(module, name, lora_linear)\n\ndef count_trainable_parameters(model):\n    \"\"\"\n    Counts and returns the number of trainable parameters in the model.\n    \"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef mark_lora_and_head_as_trainable(model: nn.Module, head_substring=\"heads.head\", bias='none'):\n    \"\"\"\n    Unfreeze LoRA parameters + the final classification head (by default `heads.head`).\n    Everything else remains frozen.\n    \"\"\"\n    for name, param in model.named_parameters():\n        # Unfreeze LoRA parameters\n        if 'lora_' in name:\n            param.requires_grad = True\n        # Unfreeze classification head\n        elif head_substring in name:\n            print(\"head_substring came:\", name)\n            param.requires_grad = True\n        else:\n            param.requires_grad = False\n\n    # Optionally allow some bias fine-tuning\n    if bias == 'all':\n        for n, p in model.named_parameters():\n            if 'bias' in n:\n                p.requires_grad = True\n    elif bias == 'lora_only':\n        for m in model.modules():\n            if isinstance(m, LoRALayer) and hasattr(m, 'bias') and m.bias is not None:\n                m.bias.requires_grad = True\n\ndef lr_lambda(current_step: int):\n    \"\"\"\n    Linear decay from step=0 to step=total_steps. At step=0 => 1.0; at step=total_steps => 0.0\n    \"\"\"\n    progress = float(current_step) / float(EPOCHS * len(train_loader))\n    return max(0.0, 1.0 - progress)\n\ndef compare_encoder_weights_consistency_with_xlinear(encoder_before, encoder_after):\n    \"\"\"\n    Compare the pretrained weights and biases of nn.Linear layers in the encoder of two models.\n    \"\"\"\n    print(\"Comparing nn.Linear weights and biases between original encoder and modified encoder...\")\n\n    for (name_before, module_before), (name_after, module_after) in zip(\n        encoder_before.named_modules(), encoder_after.named_modules()\n    ):\n        if isinstance(module_before, nn.Linear) and isinstance(module_after, xLinear):\n            if torch.equal(module_before.weight.data, module_after.weight.data):\n                pass\n            else:\n                print(f\"[MISMATCH] {name_before}: Weights differ.\")\n\n            if module_before.bias is not None and module_after.bias is not None:\n                if torch.equal(module_before.bias.data, module_after.bias.data):\n                    pass\n                else:\n                    print(f\"[MISMATCH] {name_before}: Biases differ.\")\n            elif module_before.bias is None and module_after.bias is None:\n                pass\n            else:\n                print(f\"[MISMATCH] {name_before}: One layer has bias while the other does not.\")\n\n    print(\"Comparison complete.\")\n\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    correct = 0\n    total = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels in dataloader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item() * images.size(0)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    avg_loss = total_loss / len(dataloader.dataset)\n    accuracy = 100.0 * correct / total\n    f1 = f1_score(all_labels, all_preds, average='macro')\n    return avg_loss, accuracy, f1\n\n# Dataset Preparation (CIFAR100)\nweights = ViT_B_16_Weights.IMAGENET1K_V1\npreprocess = weights.transforms()\n\ntrain_dataset = datasets.CIFAR100(\n    root='./data', \n    train=True, \n    download=True, \n    transform=preprocess\n)\ntest_dataset = datasets.CIFAR100(\n    root='./data', \n    train=False, \n    download=True, \n    transform=preprocess\n)\n\ntrain_size = 45000\nval_size = 5000\ntrain_data, val_data = torch.utils.data.random_split(\n    train_dataset, \n    [train_size, val_size]\n)\n\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Experiment loop for different LoRA ranks\nfor R_LORA in R_LORA_VALUES:\n    print(f\"\\n{'='*50}\")\n    print(f\"Running experiment with R_LORA = {R_LORA}\")\n    print(f\"{'='*50}\")\n    \n    # Initialize TensorBoard writer\n    writer = SummaryWriter(f'logs/rank_{R_LORA}')\n\n    # Model Preparation\n    model = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n    num_features = model.heads.head.in_features\n    model.heads.head = nn.Sequential(\n        nn.Dropout(DROPOUT),\n        nn.Linear(num_features, 100)   \n    )\n\n    # Apply LoRA modifications\n    replace_linear_with_lora(model)\n    mark_lora_and_head_as_trainable(model, head_substring=\"heads.head\", bias=\"none\")\n\n    # Memory footprint calculation: move model to device and reset memory stats\n    model.to(device)\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n\n    # Optimizer and scheduler\n    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.AdamW(trainable_params, lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n    # Training loop\n    criterion = nn.CrossEntropyLoss()\n    for epoch in range(EPOCHS):\n        model.train()\n        running_loss = 0.0\n\n        for step, (images, labels) in enumerate(train_loader):\n            images = images.to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            running_loss += loss.item() * images.size(0)\n\n            if step % 100 == 0:\n                current_lr = scheduler.get_last_lr()[0]\n                print(f\"[Epoch {epoch+1}/{EPOCHS} - Step {step}] Loss: {loss.item():.4f}, LR: {current_lr:.6f}\")\n\n        # Evaluation on train, validation, and test sets (now with F1 score)\n        train_loss, train_acc, train_f1 = evaluate(model, train_loader, criterion, device)\n        val_loss, val_acc, val_f1 = evaluate(model, val_loader, criterion, device)\n        test_loss, test_acc, test_f1 = evaluate(model, test_loader, criterion, device)\n\n        # Logging to TensorBoard: Loss, Accuracy, and F1 score\n        writer.add_scalar(f'Rank_{R_LORA}/Train Loss vs epoch', train_loss, epoch)\n        writer.add_scalar(f'Rank_{R_LORA}/Train Acc vs epoch', train_acc, epoch)\n        writer.add_scalar(f'Rank_{R_LORA}/Train F1 vs epoch', train_f1, epoch)\n        \n        writer.add_scalar(f'Rank_{R_LORA}/Val Loss vs epoch', val_loss, epoch)\n        writer.add_scalar(f'Rank_{R_LORA}/Val Acc vs epoch', val_acc, epoch)\n        writer.add_scalar(f'Rank_{R_LORA}/Val F1 vs epoch', val_f1, epoch)       \n        \n        writer.add_scalar(f'Rank_{R_LORA}/Test Loss vs epoch', test_loss, epoch)\n        writer.add_scalar(f'Rank_{R_LORA}/Test Acc vs epoch', test_acc, epoch)\n        writer.add_scalar(f'Rank_{R_LORA}/Test F1 vs epoch', test_f1, epoch)\n\n        print(f\"Epoch [{epoch+1}/{EPOCHS}]\")\n        print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.2f}% | F1: {train_f1:.2f}\")\n        print(f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.2f}% | F1: {val_f1:.2f}\")\n        print(f\"Test Loss: {test_loss:.4f} | Acc: {test_acc:.2f}% | F1: {test_f1:.2f}\\n\")\n\n    # Count trainable parameters\n    lora_params = sum(p.numel() for n, p in model.named_parameters() if 'lora_' in n)\n    print(f\"Number of trainable parameters: {lora_params}\")   \n\n    # After calculating num_trainable_params, measure peak memory usage\n    optimizer_memory = (3 * lora_params * 4) / (1024 ** 2)  # 4 bytes per float32, 3x for (param + moments)\n    memory_footprint = torch.cuda.max_memory_allocated() / (1024 ** 2)  # In MB\n\n    print(f\"\\nMemory Breakdown:\")\n    print(f\"Memory footprint for fine-tuning: {memory_footprint:.2f} MB\")\n    print(f\"LoRA params contribution: {optimizer_memory:.2f} MB\")\n\n    # Clean up for next experiment\n    del model\n    torch.cuda.empty_cache()\n\n    writer.close()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:41:47.368833Z","iopub.execute_input":"2025-02-01T09:41:47.369170Z","iopub.status.idle":"2025-02-01T16:51:33.753609Z","shell.execute_reply.started":"2025-02-01T09:41:47.369143Z","shell.execute_reply":"2025-02-01T16:51:33.752520Z"}},"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 169M/169M [00:05<00:00, 29.2MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-100-python.tar.gz to ./data\nFiles already downloaded and verified\n\n==================================================\nRunning experiment with R_LORA = 8\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n100%|██████████| 330M/330M [00:02<00:00, 156MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"head_substring came: heads.head.1.weight\nhead_substring came: heads.head.1.bias\n[Epoch 1/25 - Step 0] Loss: 4.6992, LR: 0.001000\n[Epoch 1/25 - Step 100] Loss: 0.6738, LR: 0.000994\n[Epoch 1/25 - Step 200] Loss: 0.6440, LR: 0.000989\n[Epoch 1/25 - Step 300] Loss: 0.5887, LR: 0.000983\n[Epoch 1/25 - Step 400] Loss: 0.6283, LR: 0.000977\n[Epoch 1/25 - Step 500] Loss: 0.8398, LR: 0.000972\n[Epoch 1/25 - Step 600] Loss: 0.8178, LR: 0.000966\n[Epoch 1/25 - Step 700] Loss: 0.5722, LR: 0.000960\nEpoch [1/25]\nTrain Loss: 0.3890 | Acc: 87.89% | F1: 0.88\nVal Loss: 0.5481 | Acc: 83.18% | F1: 0.83\nTest Loss: 0.5651 | Acc: 82.54% | F1: 0.82\n\n[Epoch 2/25 - Step 0] Loss: 0.6796, LR: 0.000960\n[Epoch 2/25 - Step 100] Loss: 0.3613, LR: 0.000954\n[Epoch 2/25 - Step 200] Loss: 0.1117, LR: 0.000949\n[Epoch 2/25 - Step 300] Loss: 0.3467, LR: 0.000943\n[Epoch 2/25 - Step 400] Loss: 0.5321, LR: 0.000937\n[Epoch 2/25 - Step 500] Loss: 0.5572, LR: 0.000932\n[Epoch 2/25 - Step 600] Loss: 0.5065, LR: 0.000926\n[Epoch 2/25 - Step 700] Loss: 0.4619, LR: 0.000920\nEpoch [2/25]\nTrain Loss: 0.2606 | Acc: 91.66% | F1: 0.92\nVal Loss: 0.5275 | Acc: 83.98% | F1: 0.84\nTest Loss: 0.5279 | Acc: 84.14% | F1: 0.84\n\n[Epoch 3/25 - Step 0] Loss: 0.1782, LR: 0.000920\n[Epoch 3/25 - Step 100] Loss: 0.2645, LR: 0.000914\n[Epoch 3/25 - Step 200] Loss: 0.0438, LR: 0.000909\n[Epoch 3/25 - Step 300] Loss: 0.3192, LR: 0.000903\n[Epoch 3/25 - Step 400] Loss: 0.3930, LR: 0.000897\n[Epoch 3/25 - Step 500] Loss: 0.3093, LR: 0.000892\n[Epoch 3/25 - Step 600] Loss: 0.4500, LR: 0.000886\n[Epoch 3/25 - Step 700] Loss: 0.1747, LR: 0.000880\nEpoch [3/25]\nTrain Loss: 0.2540 | Acc: 91.87% | F1: 0.92\nVal Loss: 0.5687 | Acc: 83.50% | F1: 0.83\nTest Loss: 0.5905 | Acc: 82.71% | F1: 0.83\n\n[Epoch 4/25 - Step 0] Loss: 0.3736, LR: 0.000880\n[Epoch 4/25 - Step 100] Loss: 0.2799, LR: 0.000874\n[Epoch 4/25 - Step 200] Loss: 0.2148, LR: 0.000869\n[Epoch 4/25 - Step 300] Loss: 0.2118, LR: 0.000863\n[Epoch 4/25 - Step 400] Loss: 0.1628, LR: 0.000857\n[Epoch 4/25 - Step 500] Loss: 0.1744, LR: 0.000852\n[Epoch 4/25 - Step 600] Loss: 0.2592, LR: 0.000846\n[Epoch 4/25 - Step 700] Loss: 0.4258, LR: 0.000840\nEpoch [4/25]\nTrain Loss: 0.1686 | Acc: 94.40% | F1: 0.94\nVal Loss: 0.5606 | Acc: 83.58% | F1: 0.83\nTest Loss: 0.5635 | Acc: 83.96% | F1: 0.84\n\n[Epoch 5/25 - Step 0] Loss: 0.2731, LR: 0.000840\n[Epoch 5/25 - Step 100] Loss: 0.1653, LR: 0.000834\n[Epoch 5/25 - Step 200] Loss: 0.2699, LR: 0.000829\n[Epoch 5/25 - Step 300] Loss: 0.1584, LR: 0.000823\n[Epoch 5/25 - Step 400] Loss: 0.1599, LR: 0.000817\n[Epoch 5/25 - Step 500] Loss: 0.1765, LR: 0.000812\n[Epoch 5/25 - Step 600] Loss: 0.2102, LR: 0.000806\n[Epoch 5/25 - Step 700] Loss: 0.2021, LR: 0.000800\nEpoch [5/25]\nTrain Loss: 0.1895 | Acc: 93.80% | F1: 0.94\nVal Loss: 0.6342 | Acc: 82.34% | F1: 0.82\nTest Loss: 0.6424 | Acc: 82.17% | F1: 0.82\n\n[Epoch 6/25 - Step 0] Loss: 0.0810, LR: 0.000800\n[Epoch 6/25 - Step 100] Loss: 0.1417, LR: 0.000794\n[Epoch 6/25 - Step 200] Loss: 0.1564, LR: 0.000789\n[Epoch 6/25 - Step 300] Loss: 0.0461, LR: 0.000783\n[Epoch 6/25 - Step 400] Loss: 0.3266, LR: 0.000777\n[Epoch 6/25 - Step 500] Loss: 0.1712, LR: 0.000772\n[Epoch 6/25 - Step 600] Loss: 0.1728, LR: 0.000766\n[Epoch 6/25 - Step 700] Loss: 0.2619, LR: 0.000760\nEpoch [6/25]\nTrain Loss: 0.1214 | Acc: 95.92% | F1: 0.96\nVal Loss: 0.6184 | Acc: 83.94% | F1: 0.84\nTest Loss: 0.5912 | Acc: 83.96% | F1: 0.84\n\n[Epoch 7/25 - Step 0] Loss: 0.1904, LR: 0.000760\n[Epoch 7/25 - Step 100] Loss: 0.1679, LR: 0.000754\n[Epoch 7/25 - Step 200] Loss: 0.0595, LR: 0.000749\n[Epoch 7/25 - Step 300] Loss: 0.1469, LR: 0.000743\n[Epoch 7/25 - Step 400] Loss: 0.1963, LR: 0.000737\n[Epoch 7/25 - Step 500] Loss: 0.0605, LR: 0.000732\n[Epoch 7/25 - Step 600] Loss: 0.0392, LR: 0.000726\n[Epoch 7/25 - Step 700] Loss: 0.0638, LR: 0.000720\nEpoch [7/25]\nTrain Loss: 0.0774 | Acc: 97.48% | F1: 0.97\nVal Loss: 0.5765 | Acc: 84.68% | F1: 0.84\nTest Loss: 0.5826 | Acc: 84.85% | F1: 0.85\n\n[Epoch 8/25 - Step 0] Loss: 0.0548, LR: 0.000720\n[Epoch 8/25 - Step 100] Loss: 0.1093, LR: 0.000714\n[Epoch 8/25 - Step 200] Loss: 0.0515, LR: 0.000709\n[Epoch 8/25 - Step 300] Loss: 0.1213, LR: 0.000703\n[Epoch 8/25 - Step 400] Loss: 0.0237, LR: 0.000697\n[Epoch 8/25 - Step 500] Loss: 0.1193, LR: 0.000692\n[Epoch 8/25 - Step 600] Loss: 0.1039, LR: 0.000686\n[Epoch 8/25 - Step 700] Loss: 0.1528, LR: 0.000680\nEpoch [8/25]\nTrain Loss: 0.1042 | Acc: 96.57% | F1: 0.97\nVal Loss: 0.6257 | Acc: 83.78% | F1: 0.84\nTest Loss: 0.6482 | Acc: 83.34% | F1: 0.83\n\n[Epoch 9/25 - Step 0] Loss: 0.0722, LR: 0.000680\n[Epoch 9/25 - Step 100] Loss: 0.1162, LR: 0.000674\n[Epoch 9/25 - Step 200] Loss: 0.0808, LR: 0.000669\n[Epoch 9/25 - Step 300] Loss: 0.1678, LR: 0.000663\n[Epoch 9/25 - Step 400] Loss: 0.2084, LR: 0.000657\n[Epoch 9/25 - Step 500] Loss: 0.0591, LR: 0.000652\n[Epoch 9/25 - Step 600] Loss: 0.0883, LR: 0.000646\n[Epoch 9/25 - Step 700] Loss: 0.0499, LR: 0.000640\nEpoch [9/25]\nTrain Loss: 0.0492 | Acc: 98.57% | F1: 0.99\nVal Loss: 0.5790 | Acc: 84.82% | F1: 0.85\nTest Loss: 0.5831 | Acc: 85.29% | F1: 0.85\n\n[Epoch 10/25 - Step 0] Loss: 0.0766, LR: 0.000640\n[Epoch 10/25 - Step 100] Loss: 0.0310, LR: 0.000634\n[Epoch 10/25 - Step 200] Loss: 0.1227, LR: 0.000629\n[Epoch 10/25 - Step 300] Loss: 0.0589, LR: 0.000623\n[Epoch 10/25 - Step 400] Loss: 0.0103, LR: 0.000617\n[Epoch 10/25 - Step 500] Loss: 0.1001, LR: 0.000612\n[Epoch 10/25 - Step 600] Loss: 0.0071, LR: 0.000606\n[Epoch 10/25 - Step 700] Loss: 0.0398, LR: 0.000600\nEpoch [10/25]\nTrain Loss: 0.0600 | Acc: 98.08% | F1: 0.98\nVal Loss: 0.6448 | Acc: 83.96% | F1: 0.84\nTest Loss: 0.6396 | Acc: 84.52% | F1: 0.84\n\n[Epoch 11/25 - Step 0] Loss: 0.0385, LR: 0.000600\n[Epoch 11/25 - Step 100] Loss: 0.0148, LR: 0.000594\n[Epoch 11/25 - Step 200] Loss: 0.0787, LR: 0.000589\n[Epoch 11/25 - Step 300] Loss: 0.0684, LR: 0.000583\n[Epoch 11/25 - Step 400] Loss: 0.1716, LR: 0.000577\n[Epoch 11/25 - Step 500] Loss: 0.0460, LR: 0.000572\n[Epoch 11/25 - Step 600] Loss: 0.0641, LR: 0.000566\n[Epoch 11/25 - Step 700] Loss: 0.0597, LR: 0.000560\nEpoch [11/25]\nTrain Loss: 0.0513 | Acc: 98.30% | F1: 0.98\nVal Loss: 0.6434 | Acc: 84.80% | F1: 0.84\nTest Loss: 0.6716 | Acc: 84.58% | F1: 0.85\n\n[Epoch 12/25 - Step 0] Loss: 0.1301, LR: 0.000560\n[Epoch 12/25 - Step 100] Loss: 0.0396, LR: 0.000554\n[Epoch 12/25 - Step 200] Loss: 0.0410, LR: 0.000549\n[Epoch 12/25 - Step 300] Loss: 0.0802, LR: 0.000543\n[Epoch 12/25 - Step 400] Loss: 0.0252, LR: 0.000537\n[Epoch 12/25 - Step 500] Loss: 0.0519, LR: 0.000532\n[Epoch 12/25 - Step 600] Loss: 0.0073, LR: 0.000526\n[Epoch 12/25 - Step 700] Loss: 0.0639, LR: 0.000520\nEpoch [12/25]\nTrain Loss: 0.0362 | Acc: 98.93% | F1: 0.99\nVal Loss: 0.6642 | Acc: 84.98% | F1: 0.85\nTest Loss: 0.6836 | Acc: 84.65% | F1: 0.85\n\n[Epoch 13/25 - Step 0] Loss: 0.0606, LR: 0.000520\n[Epoch 13/25 - Step 100] Loss: 0.0242, LR: 0.000514\n[Epoch 13/25 - Step 200] Loss: 0.0472, LR: 0.000509\n[Epoch 13/25 - Step 300] Loss: 0.0276, LR: 0.000503\n[Epoch 13/25 - Step 400] Loss: 0.0152, LR: 0.000497\n[Epoch 13/25 - Step 500] Loss: 0.0167, LR: 0.000492\n[Epoch 13/25 - Step 600] Loss: 0.0203, LR: 0.000486\n[Epoch 13/25 - Step 700] Loss: 0.0970, LR: 0.000480\nEpoch [13/25]\nTrain Loss: 0.0206 | Acc: 99.48% | F1: 0.99\nVal Loss: 0.5984 | Acc: 85.46% | F1: 0.85\nTest Loss: 0.6482 | Acc: 85.35% | F1: 0.85\n\n[Epoch 14/25 - Step 0] Loss: 0.0062, LR: 0.000480\n[Epoch 14/25 - Step 100] Loss: 0.0125, LR: 0.000474\n[Epoch 14/25 - Step 200] Loss: 0.0149, LR: 0.000469\n[Epoch 14/25 - Step 300] Loss: 0.0052, LR: 0.000463\n[Epoch 14/25 - Step 400] Loss: 0.0914, LR: 0.000457\n[Epoch 14/25 - Step 500] Loss: 0.0559, LR: 0.000452\n[Epoch 14/25 - Step 600] Loss: 0.0121, LR: 0.000446\n[Epoch 14/25 - Step 700] Loss: 0.0428, LR: 0.000440\nEpoch [14/25]\nTrain Loss: 0.0211 | Acc: 99.38% | F1: 0.99\nVal Loss: 0.6260 | Acc: 85.34% | F1: 0.85\nTest Loss: 0.6709 | Acc: 84.96% | F1: 0.85\n\n[Epoch 15/25 - Step 0] Loss: 0.0899, LR: 0.000440\n[Epoch 15/25 - Step 100] Loss: 0.1158, LR: 0.000434\n[Epoch 15/25 - Step 200] Loss: 0.0590, LR: 0.000429\n[Epoch 15/25 - Step 300] Loss: 0.0086, LR: 0.000423\n[Epoch 15/25 - Step 400] Loss: 0.0018, LR: 0.000417\n[Epoch 15/25 - Step 500] Loss: 0.0120, LR: 0.000412\n[Epoch 15/25 - Step 600] Loss: 0.0232, LR: 0.000406\n[Epoch 15/25 - Step 700] Loss: 0.0371, LR: 0.000400\nEpoch [15/25]\nTrain Loss: 0.0156 | Acc: 99.58% | F1: 1.00\nVal Loss: 0.6085 | Acc: 85.94% | F1: 0.86\nTest Loss: 0.6439 | Acc: 86.05% | F1: 0.86\n\n[Epoch 16/25 - Step 0] Loss: 0.0086, LR: 0.000400\n[Epoch 16/25 - Step 100] Loss: 0.0103, LR: 0.000394\n[Epoch 16/25 - Step 200] Loss: 0.0086, LR: 0.000389\n[Epoch 16/25 - Step 300] Loss: 0.0046, LR: 0.000383\n[Epoch 16/25 - Step 400] Loss: 0.0061, LR: 0.000377\n[Epoch 16/25 - Step 500] Loss: 0.0024, LR: 0.000372\n[Epoch 16/25 - Step 600] Loss: 0.0241, LR: 0.000366\n[Epoch 16/25 - Step 700] Loss: 0.0145, LR: 0.000360\nEpoch [16/25]\nTrain Loss: 0.0110 | Acc: 99.73% | F1: 1.00\nVal Loss: 0.6006 | Acc: 86.60% | F1: 0.86\nTest Loss: 0.6478 | Acc: 86.05% | F1: 0.86\n\n[Epoch 17/25 - Step 0] Loss: 0.0017, LR: 0.000360\n[Epoch 17/25 - Step 100] Loss: 0.0528, LR: 0.000354\n[Epoch 17/25 - Step 200] Loss: 0.0020, LR: 0.000349\n[Epoch 17/25 - Step 300] Loss: 0.0055, LR: 0.000343\n[Epoch 17/25 - Step 400] Loss: 0.0029, LR: 0.000337\n[Epoch 17/25 - Step 500] Loss: 0.0009, LR: 0.000332\n[Epoch 17/25 - Step 600] Loss: 0.0050, LR: 0.000326\n[Epoch 17/25 - Step 700] Loss: 0.0011, LR: 0.000320\nEpoch [17/25]\nTrain Loss: 0.0097 | Acc: 99.77% | F1: 1.00\nVal Loss: 0.6188 | Acc: 85.60% | F1: 0.85\nTest Loss: 0.6668 | Acc: 85.82% | F1: 0.86\n\n[Epoch 18/25 - Step 0] Loss: 0.0052, LR: 0.000320\n[Epoch 18/25 - Step 100] Loss: 0.0006, LR: 0.000314\n[Epoch 18/25 - Step 200] Loss: 0.0008, LR: 0.000309\n[Epoch 18/25 - Step 300] Loss: 0.0013, LR: 0.000303\n[Epoch 18/25 - Step 400] Loss: 0.0026, LR: 0.000297\n[Epoch 18/25 - Step 500] Loss: 0.0079, LR: 0.000292\n[Epoch 18/25 - Step 600] Loss: 0.0045, LR: 0.000286\n[Epoch 18/25 - Step 700] Loss: 0.0023, LR: 0.000280\nEpoch [18/25]\nTrain Loss: 0.0031 | Acc: 99.94% | F1: 1.00\nVal Loss: 0.5992 | Acc: 86.86% | F1: 0.87\nTest Loss: 0.6401 | Acc: 86.51% | F1: 0.86\n\n[Epoch 19/25 - Step 0] Loss: 0.0023, LR: 0.000280\n[Epoch 19/25 - Step 100] Loss: 0.0002, LR: 0.000274\n[Epoch 19/25 - Step 200] Loss: 0.0015, LR: 0.000269\n[Epoch 19/25 - Step 300] Loss: 0.0002, LR: 0.000263\n[Epoch 19/25 - Step 400] Loss: 0.0217, LR: 0.000257\n[Epoch 19/25 - Step 500] Loss: 0.0016, LR: 0.000252\n[Epoch 19/25 - Step 600] Loss: 0.0006, LR: 0.000246\n[Epoch 19/25 - Step 700] Loss: 0.0012, LR: 0.000240\nEpoch [19/25]\nTrain Loss: 0.0014 | Acc: 99.97% | F1: 1.00\nVal Loss: 0.5950 | Acc: 87.26% | F1: 0.87\nTest Loss: 0.6363 | Acc: 86.96% | F1: 0.87\n\n[Epoch 20/25 - Step 0] Loss: 0.0010, LR: 0.000240\n[Epoch 20/25 - Step 100] Loss: 0.0004, LR: 0.000234\n[Epoch 20/25 - Step 200] Loss: 0.0004, LR: 0.000229\n[Epoch 20/25 - Step 300] Loss: 0.0008, LR: 0.000223\n[Epoch 20/25 - Step 400] Loss: 0.0005, LR: 0.000217\n[Epoch 20/25 - Step 500] Loss: 0.0005, LR: 0.000212\n[Epoch 20/25 - Step 600] Loss: 0.0004, LR: 0.000206\n[Epoch 20/25 - Step 700] Loss: 0.0004, LR: 0.000200\nEpoch [20/25]\nTrain Loss: 0.0020 | Acc: 99.96% | F1: 1.00\nVal Loss: 0.5982 | Acc: 87.90% | F1: 0.88\nTest Loss: 0.6390 | Acc: 87.05% | F1: 0.87\n\n[Epoch 21/25 - Step 0] Loss: 0.0006, LR: 0.000200\n[Epoch 21/25 - Step 100] Loss: 0.0006, LR: 0.000194\n[Epoch 21/25 - Step 200] Loss: 0.0001, LR: 0.000189\n[Epoch 21/25 - Step 300] Loss: 0.0003, LR: 0.000183\n[Epoch 21/25 - Step 400] Loss: 0.0009, LR: 0.000177\n[Epoch 21/25 - Step 500] Loss: 0.0322, LR: 0.000172\n[Epoch 21/25 - Step 600] Loss: 0.0004, LR: 0.000166\n[Epoch 21/25 - Step 700] Loss: 0.0005, LR: 0.000160\nEpoch [21/25]\nTrain Loss: 0.0011 | Acc: 99.98% | F1: 1.00\nVal Loss: 0.6052 | Acc: 87.22% | F1: 0.87\nTest Loss: 0.6480 | Acc: 86.89% | F1: 0.87\n\n[Epoch 22/25 - Step 0] Loss: 0.0003, LR: 0.000160\n[Epoch 22/25 - Step 100] Loss: 0.0003, LR: 0.000154\n[Epoch 22/25 - Step 200] Loss: 0.0004, LR: 0.000149\n[Epoch 22/25 - Step 300] Loss: 0.0002, LR: 0.000143\n[Epoch 22/25 - Step 400] Loss: 0.0004, LR: 0.000137\n[Epoch 22/25 - Step 500] Loss: 0.0003, LR: 0.000132\n[Epoch 22/25 - Step 600] Loss: 0.0002, LR: 0.000126\n[Epoch 22/25 - Step 700] Loss: 0.0005, LR: 0.000120\nEpoch [22/25]\nTrain Loss: 0.0008 | Acc: 99.98% | F1: 1.00\nVal Loss: 0.6097 | Acc: 87.36% | F1: 0.87\nTest Loss: 0.6416 | Acc: 87.05% | F1: 0.87\n\n[Epoch 23/25 - Step 0] Loss: 0.0006, LR: 0.000120\n[Epoch 23/25 - Step 100] Loss: 0.0003, LR: 0.000114\n[Epoch 23/25 - Step 200] Loss: 0.0003, LR: 0.000109\n[Epoch 23/25 - Step 300] Loss: 0.0002, LR: 0.000103\n[Epoch 23/25 - Step 400] Loss: 0.0004, LR: 0.000097\n[Epoch 23/25 - Step 500] Loss: 0.0002, LR: 0.000092\n[Epoch 23/25 - Step 600] Loss: 0.0002, LR: 0.000086\n[Epoch 23/25 - Step 700] Loss: 0.0463, LR: 0.000080\nEpoch [23/25]\nTrain Loss: 0.0005 | Acc: 99.98% | F1: 1.00\nVal Loss: 0.6069 | Acc: 87.70% | F1: 0.88\nTest Loss: 0.6463 | Acc: 87.19% | F1: 0.87\n\n[Epoch 24/25 - Step 0] Loss: 0.0001, LR: 0.000080\n[Epoch 24/25 - Step 100] Loss: 0.0002, LR: 0.000074\n[Epoch 24/25 - Step 200] Loss: 0.0003, LR: 0.000069\n[Epoch 24/25 - Step 300] Loss: 0.0001, LR: 0.000063\n[Epoch 24/25 - Step 400] Loss: 0.0000, LR: 0.000057\n[Epoch 24/25 - Step 500] Loss: 0.0002, LR: 0.000052\n[Epoch 24/25 - Step 600] Loss: 0.0002, LR: 0.000046\n[Epoch 24/25 - Step 700] Loss: 0.0002, LR: 0.000040\nEpoch [24/25]\nTrain Loss: 0.0004 | Acc: 99.98% | F1: 1.00\nVal Loss: 0.6102 | Acc: 87.50% | F1: 0.87\nTest Loss: 0.6491 | Acc: 87.13% | F1: 0.87\n\n[Epoch 25/25 - Step 0] Loss: 0.0002, LR: 0.000040\n[Epoch 25/25 - Step 100] Loss: 0.0002, LR: 0.000034\n[Epoch 25/25 - Step 200] Loss: 0.0002, LR: 0.000029\n[Epoch 25/25 - Step 300] Loss: 0.0002, LR: 0.000023\n[Epoch 25/25 - Step 400] Loss: 0.0110, LR: 0.000017\n[Epoch 25/25 - Step 500] Loss: 0.0001, LR: 0.000012\n[Epoch 25/25 - Step 600] Loss: 0.0002, LR: 0.000006\n[Epoch 25/25 - Step 700] Loss: 0.0001, LR: 0.000000\nEpoch [25/25]\nTrain Loss: 0.0004 | Acc: 99.98% | F1: 1.00\nVal Loss: 0.6118 | Acc: 87.74% | F1: 0.88\nTest Loss: 0.6499 | Acc: 87.17% | F1: 0.87\n\nNumber of trainable parameters: 884736\n\nMemory Breakdown:\nMemory footprint for fine-tuning: 7176.87 MB\nLoRA params contribution: 10.12 MB\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import shutil\n\n# Define the folder to zip and the output zip file name\nfolder_to_zip = '/kaggle/working/logs'\noutput_zip = '/kaggle/working/logs_cifar100_rank8.zip'\n\n# Create a zip file\nshutil.make_archive(output_zip.replace('.zip', ''), 'zip', folder_to_zip)\n\nprint(f\"Zipped folder is saved as {output_zip}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T16:51:33.794764Z","iopub.execute_input":"2025-02-01T16:51:33.795100Z","iopub.status.idle":"2025-02-01T16:51:33.806389Z","shell.execute_reply.started":"2025-02-01T16:51:33.795069Z","shell.execute_reply":"2025-02-01T16:51:33.805641Z"}},"outputs":[{"name":"stdout","text":"Zipped folder is saved as /kaggle/working/logs_cifar100_rank8.zip\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"         b               ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}