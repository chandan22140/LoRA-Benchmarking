{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"                     ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nfrom torchvision.models import vit_b_16, ViT_B_16_Weights\nfrom torch.utils.tensorboard import SummaryWriter\nimport time\nimport copy\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nfrom torchvision.models import vit_b_16, ViT_B_16_Weights\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms, datasets, models\nfrom torch.utils.data import DataLoader\nfrom torchvision.models import vit_b_16, ViT_B_16_Weights\nfrom typing import Dict\nimport math\nfrom typing import Optional, List\n\nimport time\nimport copy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\n# Import f1_score for F1 calculation\nfrom sklearn.metrics import f1_score\n\n# Hyperparameters\nEPOCHS = 15\nbatch_size = 64\nBASE_LR = 1e-3\nWEIGHT_DECAY = 0.03\nDROPOUT = 0.1\nLORA_ALPHA = 32\nLORA_DROPOUT = 0\nR_LORA_VALUES = [1, 4, 8]  # LoRA ranks to evaluate\n\n\nclass LoRALayer():\n    def __init__(\n        self,\n        r: int,\n        lora_alpha: int,\n        lora_dropout: float,\n        merge_weights: bool,\n    ):\n        self.r = r\n        self.lora_alpha = lora_alpha\n\n        # Optional dropout\n        if lora_dropout > 0.:\n            self.lora_dropout = nn.Dropout(p=lora_dropout)\n        else:\n            self.lora_dropout = lambda x: x\n        # Mark the weight as unmerged\n        self.merged = False\n        self.merge_weights = merge_weights\n\n\nclass xLinear(nn.Linear, LoRALayer):\n    # LoRA implemented in a dense layer\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        r: int = 0,\n        lora_alpha: int = 32,\n        lora_dropout: float = 0.0,\n        fan_in_fan_out: bool = False,\n        merge_weights: bool = True,\n        pretrained_weights=None,  # Added to accept pretrained weights\n        pretrained_bias=None,     # Added to accept pretrained bias\n        **kwargs\n    ):\n        super().__init__(in_features, out_features, **kwargs)\n        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n                           merge_weights=merge_weights)\n\n        self.fan_in_fan_out = fan_in_fan_out\n        if pretrained_weights is not None:\n            self.weight.data = pretrained_weights\n        if pretrained_bias is not None:\n            self.bias.data = pretrained_bias\n\n        # Actual trainable parameters\n        if r > 0:\n            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))\n            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))\n            self.scaling = self.lora_alpha / self.r\n            self.weight.requires_grad = False\n        self._initialize_lora_parameters()  # Only initialize LoRA parameters\n        if fan_in_fan_out:\n            self.weight.data = self.weight.data.transpose(0, 1)\n\n    def _initialize_lora_parameters(self):\n        \"\"\"\n        Initialize only the LoRA-specific parameters (lora_A and lora_B).\n        Avoid reinitializing self.weight or self.bias to preserve pretrained values.\n        \"\"\"\n        if hasattr(self, 'lora_A'):\n            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n            nn.init.zeros_(self.lora_B)\n            \n    def train(self, mode: bool = True):\n        def T(w):\n            return w.transpose(0, 1) if self.fan_in_fan_out else w\n        nn.Linear.train(self, mode)\n        if mode:\n            if self.merge_weights and self.merged:\n                # Make sure that the weights are not merged\n                if self.r > 0:\n                    self.weight.data -= T(self.lora_B @ self.lora_A) * self.scaling\n                self.merged = False\n        else:\n            if self.merge_weights and not self.merged:\n                # Merge the weights and mark it\n                if self.r > 0:\n                    self.weight.data += T(self.lora_B @ self.lora_A) * self.scaling\n                self.merged = True\n\n    def forward(self, x: torch.Tensor):\n        def T(w):\n            return w.transpose(0, 1) if self.fan_in_fan_out else w\n        if self.r > 0 and not self.merged:\n            result = F.linear(x, T(self.weight), bias=self.bias)\n            result += (self.lora_dropout(x) @ self.lora_A.transpose(0, 1) @ self.lora_B.transpose(0, 1)) * self.scaling\n            return result\n        else:\n            return F.linear(x, T(self.weight), bias=self.bias)\n\ndef replace_linear_with_lora(module: nn.Module, parent_name='', skip_substring='heads.head'):\n    \"\"\"\n    Recursively replace all nn.Linear modules with LoRALayer.Linear,\n    while preserving pretrained weights and biases and skipping specific submodules.\n    \"\"\"\n    for name, child in list(module.named_children()):\n        # Form the fully qualified name (like 'encoder.layer1.linear')\n        module_path = f\"{parent_name}.{name}\" if parent_name else name\n\n        # Recursively apply to child modules first\n        replace_linear_with_lora(child, parent_name=module_path, skip_substring=skip_substring)\n\n        if isinstance(child, nn.Linear) and skip_substring not in module_path:\n            # Extract pretrained weights and bias\n            pretrained_weights = child.weight.data.clone()\n            pretrained_bias = child.bias.data.clone() if child.bias is not None else None\n\n            # Replace the nn.Linear with LoRA-wrapped Linear\n            lora_linear = xLinear(\n                in_features=child.in_features,\n                out_features=child.out_features,\n                r=R_LORA,\n                lora_alpha=LORA_ALPHA,\n                lora_dropout=LORA_DROPOUT,\n                pretrained_weights=pretrained_weights,\n                pretrained_bias=pretrained_bias,\n            )\n            setattr(module, name, lora_linear)\n\ndef count_trainable_parameters(model):\n    \"\"\"\n    Counts and returns the number of trainable parameters in the model.\n    \"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef mark_lora_and_head_as_trainable(model: nn.Module, head_substring=\"heads.head\", bias='none'):\n    \"\"\"\n    Unfreeze LoRA parameters + the final classification head (by default `heads.head`).\n    Everything else remains frozen.\n    \"\"\"\n    for name, param in model.named_parameters():\n        # Unfreeze LoRA parameters\n        if 'lora_' in name:\n            param.requires_grad = True\n        # Unfreeze classification head\n        elif head_substring in name:\n            print(\"head_substring came:\", name)\n            param.requires_grad = True\n        else:\n            param.requires_grad = False\n\n    # Optionally allow some bias fine-tuning\n    if bias == 'all':\n        for n, p in model.named_parameters():\n            if 'bias' in n:\n                p.requires_grad = True\n    elif bias == 'lora_only':\n        for m in model.modules():\n            if isinstance(m, LoRALayer) and hasattr(m, 'bias') and m.bias is not None:\n                m.bias.requires_grad = True\n\ndef lr_lambda(current_step: int):\n    \"\"\"\n    Linear decay from step=0 to step=total_steps. At step=0 => 1.0; at step=total_steps => 0.0\n    \"\"\"\n    progress = float(current_step) / float(EPOCHS * len(train_loader))\n    return max(0.0, 1.0 - progress)\n\ndef compare_encoder_weights_consistency_with_xlinear(encoder_before, encoder_after):\n    \"\"\"\n    Compare the pretrained weights and biases of nn.Linear layers in the encoder of two models.\n    \"\"\"\n    print(\"Comparing nn.Linear weights and biases between original encoder and modified encoder...\")\n\n    for (name_before, module_before), (name_after, module_after) in zip(\n        encoder_before.named_modules(), encoder_after.named_modules()\n    ):\n        if isinstance(module_before, nn.Linear) and isinstance(module_after, xLinear):\n            if torch.equal(module_before.weight.data, module_after.weight.data):\n                pass\n            else:\n                print(f\"[MISMATCH] {name_before}: Weights differ.\")\n\n            if module_before.bias is not None and module_after.bias is not None:\n                if torch.equal(module_before.bias.data, module_after.bias.data):\n                    pass\n                else:\n                    print(f\"[MISMATCH] {name_before}: Biases differ.\")\n            elif module_before.bias is None and module_after.bias is None:\n                pass\n            else:\n                print(f\"[MISMATCH] {name_before}: One layer has bias while the other does not.\")\n\n    print(\"Comparison complete.\")\n\ndef evaluate(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    correct = 0\n    total = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels in dataloader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item() * images.size(0)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    avg_loss = total_loss / len(dataloader.dataset)\n    accuracy = 100.0 * correct / total\n    f1 = f1_score(all_labels, all_preds, average='macro')\n    return avg_loss, accuracy, f1\n\n# Dataset Preparation (CIFAR100)\nweights = ViT_B_16_Weights.IMAGENET1K_V1\npreprocess = weights.transforms()\n\ntrain_dataset = datasets.CIFAR100(\n    root='./data', \n    train=True, \n    download=True, \n    transform=preprocess\n)\ntest_dataset = datasets.CIFAR100(\n    root='./data', \n    train=False, \n    download=True, \n    transform=preprocess\n)\n\ntrain_size = 45000\nval_size = 5000\ntrain_data, val_data = torch.utils.data.random_split(\n    train_dataset, \n    [train_size, val_size]\n)\n\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Experiment loop for different LoRA ranks\nfor R_LORA in R_LORA_VALUES:\n    print(f\"\\n{'='*50}\")\n    print(f\"Running experiment with R_LORA = {R_LORA}\")\n    print(f\"{'='*50}\")\n    \n    # Initialize TensorBoard writer\n    writer = SummaryWriter(f'logs/rank_{R_LORA}')\n\n    # Model Preparation\n    model = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n    num_features = model.heads.head.in_features\n    model.heads.head = nn.Sequential(\n        nn.Dropout(DROPOUT),\n        nn.Linear(num_features, 100)   \n    )\n\n    # Apply LoRA modifications\n    replace_linear_with_lora(model)\n    mark_lora_and_head_as_trainable(model, head_substring=\"heads.head\", bias=\"none\")\n\n    # Memory footprint calculation: move model to device and reset memory stats\n    model.to(device)\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n\n    # Optimizer and scheduler\n    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.AdamW(trainable_params, lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n    # Training loop\n    criterion = nn.CrossEntropyLoss()\n    for epoch in range(EPOCHS):\n        model.train()\n        running_loss = 0.0\n\n        for step, (images, labels) in enumerate(train_loader):\n            images = images.to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n\n            running_loss += loss.item() * images.size(0)\n\n            if step % 100 == 0:\n                current_lr = scheduler.get_last_lr()[0]\n                print(f\"[Epoch {epoch+1}/{EPOCHS} - Step {step}] Loss: {loss.item():.4f}, LR: {current_lr:.6f}\")\n\n        # Evaluation on train, validation, and test sets (now with F1 score)\n        train_loss, train_acc, train_f1 = evaluate(model, train_loader, criterion, device)\n        val_loss, val_acc, val_f1 = evaluate(model, val_loader, criterion, device)\n        test_loss, test_acc, test_f1 = evaluate(model, test_loader, criterion, device)\n\n        # Logging to TensorBoard: Loss, Accuracy, and F1 score\n        writer.add_scalar(f'Rank_{R_LORA}/Train Loss vs epoch', train_loss, epoch)\n        writer.add_scalar(f'Rank_{R_LORA}/Train Acc vs epoch', train_acc, epoch)\n        writer.add_scalar(f'Rank_{R_LORA}/Train F1 vs epoch', train_f1, epoch)\n        \n        writer.add_scalar(f'Rank_{R_LORA}/Val Loss vs epoch', val_loss, epoch)\n        writer.add_scalar(f'Rank_{R_LORA}/Val Acc vs epoch', val_acc, epoch)\n        writer.add_scalar(f'Rank_{R_LORA}/Val F1 vs epoch', val_f1, epoch)       \n        \n        writer.add_scalar(f'Rank_{R_LORA}/Test Loss vs epoch', test_loss, epoch)\n        writer.add_scalar(f'Rank_{R_LORA}/Test Acc vs epoch', test_acc, epoch)\n        writer.add_scalar(f'Rank_{R_LORA}/Test F1 vs epoch', test_f1, epoch)\n\n        print(f\"Epoch [{epoch+1}/{EPOCHS}]\")\n        print(f\"Train Loss: {train_loss:.4f} | Acc: {train_acc:.2f}% | F1: {train_f1:.2f}\")\n        print(f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.2f}% | F1: {val_f1:.2f}\")\n        print(f\"Test Loss: {test_loss:.4f} | Acc: {test_acc:.2f}% | F1: {test_f1:.2f}\\n\")\n\n    # Count trainable parameters\n    lora_params = sum(p.numel() for n, p in model.named_parameters() if 'lora_' in n)\n    print(f\"Number of trainable parameters: {lora_params}\")   \n\n    # After calculating num_trainable_params, measure peak memory usage\n    optimizer_memory = (3 * lora_params * 4) / (1024 ** 2)  # 4 bytes per float32, 3x for (param + moments)\n    memory_footprint = torch.cuda.max_memory_allocated() / (1024 ** 2)  # In MB\n\n    print(f\"\\nMemory Breakdown:\")\n    print(f\"Memory footprint for fine-tuning: {memory_footprint:.2f} MB\")\n    print(f\"LoRA params contribution: {optimizer_memory:.2f} MB\")\n\n    # Clean up for next experiment\n    del model\n    torch.cuda.empty_cache()\n\n    writer.close()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-31T22:45:13.156795Z","iopub.execute_input":"2025-01-31T22:45:13.157070Z","iopub.status.idle":"2025-02-01T09:38:12.767294Z","shell.execute_reply.started":"2025-01-31T22:45:13.157049Z","shell.execute_reply":"2025-02-01T09:38:12.765862Z"}},"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 169M/169M [00:04<00:00, 36.9MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-100-python.tar.gz to ./data\nFiles already downloaded and verified\n\n==================================================\nRunning experiment with R_LORA = 1\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n100%|██████████| 330M/330M [00:04<00:00, 80.7MB/s] \n","output_type":"stream"},{"name":"stdout","text":"head_substring came: heads.head.1.weight\nhead_substring came: heads.head.1.bias\n[Epoch 1/15 - Step 0] Loss: 4.6207, LR: 0.001000\n[Epoch 1/15 - Step 100] Loss: 1.1009, LR: 0.000990\n[Epoch 1/15 - Step 200] Loss: 0.8598, LR: 0.000981\n[Epoch 1/15 - Step 300] Loss: 0.7668, LR: 0.000971\n[Epoch 1/15 - Step 400] Loss: 0.6762, LR: 0.000962\n[Epoch 1/15 - Step 500] Loss: 0.4378, LR: 0.000953\n[Epoch 1/15 - Step 600] Loss: 0.5291, LR: 0.000943\n[Epoch 1/15 - Step 700] Loss: 0.7642, LR: 0.000934\nEpoch [1/15]\nTrain Loss: 0.4747 | Acc: 85.23% | F1: 0.85\nVal Loss: 0.5957 | Acc: 81.82% | F1: 0.81\nTest Loss: 0.6002 | Acc: 81.42% | F1: 0.81\n\n[Epoch 2/15 - Step 0] Loss: 0.3516, LR: 0.000933\n[Epoch 2/15 - Step 100] Loss: 0.3065, LR: 0.000924\n[Epoch 2/15 - Step 200] Loss: 0.4609, LR: 0.000914\n[Epoch 2/15 - Step 300] Loss: 0.8345, LR: 0.000905\n[Epoch 2/15 - Step 400] Loss: 0.6483, LR: 0.000895\n[Epoch 2/15 - Step 500] Loss: 0.5512, LR: 0.000886\n[Epoch 2/15 - Step 600] Loss: 0.5092, LR: 0.000876\n[Epoch 2/15 - Step 700] Loss: 0.4909, LR: 0.000867\nEpoch [2/15]\nTrain Loss: 0.3638 | Acc: 88.51% | F1: 0.88\nVal Loss: 0.5400 | Acc: 83.66% | F1: 0.83\nTest Loss: 0.5455 | Acc: 83.27% | F1: 0.83\n\n[Epoch 3/15 - Step 0] Loss: 0.3561, LR: 0.000867\n[Epoch 3/15 - Step 100] Loss: 0.2631, LR: 0.000857\n[Epoch 3/15 - Step 200] Loss: 0.3946, LR: 0.000848\n[Epoch 3/15 - Step 300] Loss: 0.5993, LR: 0.000838\n[Epoch 3/15 - Step 400] Loss: 0.5532, LR: 0.000829\n[Epoch 3/15 - Step 500] Loss: 0.2913, LR: 0.000819\n[Epoch 3/15 - Step 600] Loss: 0.5158, LR: 0.000810\n[Epoch 3/15 - Step 700] Loss: 0.3817, LR: 0.000800\nEpoch [3/15]\nTrain Loss: 0.3177 | Acc: 90.14% | F1: 0.90\nVal Loss: 0.5244 | Acc: 83.96% | F1: 0.84\nTest Loss: 0.5371 | Acc: 83.77% | F1: 0.84\n\n[Epoch 4/15 - Step 0] Loss: 0.2649, LR: 0.000800\n[Epoch 4/15 - Step 100] Loss: 0.1854, LR: 0.000790\n[Epoch 4/15 - Step 200] Loss: 0.3530, LR: 0.000781\n[Epoch 4/15 - Step 300] Loss: 0.1698, LR: 0.000771\n[Epoch 4/15 - Step 400] Loss: 0.2359, LR: 0.000762\n[Epoch 4/15 - Step 500] Loss: 0.2438, LR: 0.000753\n[Epoch 4/15 - Step 600] Loss: 0.3996, LR: 0.000743\n[Epoch 4/15 - Step 700] Loss: 0.2017, LR: 0.000734\nEpoch [4/15]\nTrain Loss: 0.2367 | Acc: 92.56% | F1: 0.93\nVal Loss: 0.4947 | Acc: 84.88% | F1: 0.85\nTest Loss: 0.5073 | Acc: 84.88% | F1: 0.85\n\n[Epoch 5/15 - Step 0] Loss: 0.1208, LR: 0.000733\n[Epoch 5/15 - Step 100] Loss: 0.2841, LR: 0.000724\n[Epoch 5/15 - Step 200] Loss: 0.2399, LR: 0.000714\n[Epoch 5/15 - Step 300] Loss: 0.2798, LR: 0.000705\n[Epoch 5/15 - Step 400] Loss: 0.2718, LR: 0.000695\n[Epoch 5/15 - Step 500] Loss: 0.1769, LR: 0.000686\n[Epoch 5/15 - Step 600] Loss: 0.2302, LR: 0.000676\n[Epoch 5/15 - Step 700] Loss: 0.2542, LR: 0.000667\nEpoch [5/15]\nTrain Loss: 0.1979 | Acc: 94.03% | F1: 0.94\nVal Loss: 0.4749 | Acc: 85.84% | F1: 0.86\nTest Loss: 0.5013 | Acc: 84.68% | F1: 0.85\n\n[Epoch 6/15 - Step 0] Loss: 0.1681, LR: 0.000667\n[Epoch 6/15 - Step 100] Loss: 0.1884, LR: 0.000657\n[Epoch 6/15 - Step 200] Loss: 0.0722, LR: 0.000648\n[Epoch 6/15 - Step 300] Loss: 0.2185, LR: 0.000638\n[Epoch 6/15 - Step 400] Loss: 0.2383, LR: 0.000629\n[Epoch 6/15 - Step 500] Loss: 0.2788, LR: 0.000619\n[Epoch 6/15 - Step 600] Loss: 0.2528, LR: 0.000610\n[Epoch 6/15 - Step 700] Loss: 0.2891, LR: 0.000600\nEpoch [6/15]\nTrain Loss: 0.1693 | Acc: 94.98% | F1: 0.95\nVal Loss: 0.4985 | Acc: 85.22% | F1: 0.85\nTest Loss: 0.5121 | Acc: 84.68% | F1: 0.85\n\n[Epoch 7/15 - Step 0] Loss: 0.2488, LR: 0.000600\n[Epoch 7/15 - Step 100] Loss: 0.1422, LR: 0.000590\n[Epoch 7/15 - Step 200] Loss: 0.2978, LR: 0.000581\n[Epoch 7/15 - Step 300] Loss: 0.2598, LR: 0.000571\n[Epoch 7/15 - Step 400] Loss: 0.2149, LR: 0.000562\n[Epoch 7/15 - Step 500] Loss: 0.2207, LR: 0.000553\n[Epoch 7/15 - Step 600] Loss: 0.3153, LR: 0.000543\n[Epoch 7/15 - Step 700] Loss: 0.3363, LR: 0.000534\nEpoch [7/15]\nTrain Loss: 0.1424 | Acc: 95.82% | F1: 0.96\nVal Loss: 0.5045 | Acc: 85.50% | F1: 0.85\nTest Loss: 0.5210 | Acc: 84.69% | F1: 0.85\n\n[Epoch 8/15 - Step 0] Loss: 0.1936, LR: 0.000533\n[Epoch 8/15 - Step 100] Loss: 0.1167, LR: 0.000524\n[Epoch 8/15 - Step 200] Loss: 0.1578, LR: 0.000514\n[Epoch 8/15 - Step 300] Loss: 0.1253, LR: 0.000505\n[Epoch 8/15 - Step 400] Loss: 0.2693, LR: 0.000495\n[Epoch 8/15 - Step 500] Loss: 0.1046, LR: 0.000486\n[Epoch 8/15 - Step 600] Loss: 0.2165, LR: 0.000476\n[Epoch 8/15 - Step 700] Loss: 0.1426, LR: 0.000467\nEpoch [8/15]\nTrain Loss: 0.1028 | Acc: 97.18% | F1: 0.97\nVal Loss: 0.4837 | Acc: 85.78% | F1: 0.85\nTest Loss: 0.4923 | Acc: 85.84% | F1: 0.86\n\n[Epoch 9/15 - Step 0] Loss: 0.0792, LR: 0.000467\n[Epoch 9/15 - Step 100] Loss: 0.0923, LR: 0.000457\n[Epoch 9/15 - Step 200] Loss: 0.1274, LR: 0.000448\n[Epoch 9/15 - Step 300] Loss: 0.0575, LR: 0.000438\n[Epoch 9/15 - Step 400] Loss: 0.1577, LR: 0.000429\n[Epoch 9/15 - Step 500] Loss: 0.2006, LR: 0.000419\n[Epoch 9/15 - Step 600] Loss: 0.1041, LR: 0.000410\n[Epoch 9/15 - Step 700] Loss: 0.1741, LR: 0.000400\nEpoch [9/15]\nTrain Loss: 0.0760 | Acc: 98.18% | F1: 0.98\nVal Loss: 0.4818 | Acc: 86.02% | F1: 0.86\nTest Loss: 0.4999 | Acc: 85.74% | F1: 0.86\n\n[Epoch 10/15 - Step 0] Loss: 0.1062, LR: 0.000400\n[Epoch 10/15 - Step 100] Loss: 0.0925, LR: 0.000390\n[Epoch 10/15 - Step 200] Loss: 0.1463, LR: 0.000381\n[Epoch 10/15 - Step 300] Loss: 0.0959, LR: 0.000371\n[Epoch 10/15 - Step 400] Loss: 0.1376, LR: 0.000362\n[Epoch 10/15 - Step 500] Loss: 0.0852, LR: 0.000353\n[Epoch 10/15 - Step 600] Loss: 0.1191, LR: 0.000343\n[Epoch 10/15 - Step 700] Loss: 0.0966, LR: 0.000334\nEpoch [10/15]\nTrain Loss: 0.0536 | Acc: 98.90% | F1: 0.99\nVal Loss: 0.4900 | Acc: 86.38% | F1: 0.86\nTest Loss: 0.5064 | Acc: 85.99% | F1: 0.86\n\n[Epoch 11/15 - Step 0] Loss: 0.0456, LR: 0.000333\n[Epoch 11/15 - Step 100] Loss: 0.0592, LR: 0.000324\n[Epoch 11/15 - Step 200] Loss: 0.0229, LR: 0.000314\n[Epoch 11/15 - Step 300] Loss: 0.1110, LR: 0.000305\n[Epoch 11/15 - Step 400] Loss: 0.0890, LR: 0.000295\n[Epoch 11/15 - Step 500] Loss: 0.0705, LR: 0.000286\n[Epoch 11/15 - Step 600] Loss: 0.1118, LR: 0.000276\n[Epoch 11/15 - Step 700] Loss: 0.0647, LR: 0.000267\nEpoch [11/15]\nTrain Loss: 0.0405 | Acc: 99.30% | F1: 0.99\nVal Loss: 0.4977 | Acc: 86.50% | F1: 0.86\nTest Loss: 0.5179 | Acc: 85.94% | F1: 0.86\n\n[Epoch 12/15 - Step 0] Loss: 0.1155, LR: 0.000267\n[Epoch 12/15 - Step 100] Loss: 0.0263, LR: 0.000257\n[Epoch 12/15 - Step 200] Loss: 0.0499, LR: 0.000248\n[Epoch 12/15 - Step 300] Loss: 0.0320, LR: 0.000238\n[Epoch 12/15 - Step 400] Loss: 0.0443, LR: 0.000229\n[Epoch 12/15 - Step 500] Loss: 0.0211, LR: 0.000219\n[Epoch 12/15 - Step 600] Loss: 0.0353, LR: 0.000210\n[Epoch 12/15 - Step 700] Loss: 0.0397, LR: 0.000200\nEpoch [12/15]\nTrain Loss: 0.0252 | Acc: 99.73% | F1: 1.00\nVal Loss: 0.5086 | Acc: 86.74% | F1: 0.86\nTest Loss: 0.5181 | Acc: 86.20% | F1: 0.86\n\n[Epoch 13/15 - Step 0] Loss: 0.0452, LR: 0.000200\n[Epoch 13/15 - Step 100] Loss: 0.0109, LR: 0.000190\n[Epoch 13/15 - Step 200] Loss: 0.0229, LR: 0.000181\n[Epoch 13/15 - Step 300] Loss: 0.0608, LR: 0.000171\n[Epoch 13/15 - Step 400] Loss: 0.0433, LR: 0.000162\n[Epoch 13/15 - Step 500] Loss: 0.0414, LR: 0.000153\n[Epoch 13/15 - Step 600] Loss: 0.0360, LR: 0.000143\n[Epoch 13/15 - Step 700] Loss: 0.0205, LR: 0.000134\nEpoch [13/15]\nTrain Loss: 0.0174 | Acc: 99.85% | F1: 1.00\nVal Loss: 0.5173 | Acc: 86.84% | F1: 0.87\nTest Loss: 0.5332 | Acc: 86.18% | F1: 0.86\n\n[Epoch 14/15 - Step 0] Loss: 0.0118, LR: 0.000133\n[Epoch 14/15 - Step 100] Loss: 0.0228, LR: 0.000124\n[Epoch 14/15 - Step 200] Loss: 0.0261, LR: 0.000114\n[Epoch 14/15 - Step 300] Loss: 0.0166, LR: 0.000105\n[Epoch 14/15 - Step 400] Loss: 0.0232, LR: 0.000095\n[Epoch 14/15 - Step 500] Loss: 0.0067, LR: 0.000086\n[Epoch 14/15 - Step 600] Loss: 0.0169, LR: 0.000076\n[Epoch 14/15 - Step 700] Loss: 0.0176, LR: 0.000067\nEpoch [14/15]\nTrain Loss: 0.0127 | Acc: 99.91% | F1: 1.00\nVal Loss: 0.5300 | Acc: 86.82% | F1: 0.87\nTest Loss: 0.5430 | Acc: 86.13% | F1: 0.86\n\n[Epoch 15/15 - Step 0] Loss: 0.0147, LR: 0.000067\n[Epoch 15/15 - Step 100] Loss: 0.0108, LR: 0.000057\n[Epoch 15/15 - Step 200] Loss: 0.0114, LR: 0.000048\n[Epoch 15/15 - Step 300] Loss: 0.0106, LR: 0.000038\n[Epoch 15/15 - Step 400] Loss: 0.0138, LR: 0.000029\n[Epoch 15/15 - Step 500] Loss: 0.0121, LR: 0.000019\n[Epoch 15/15 - Step 600] Loss: 0.0181, LR: 0.000010\n[Epoch 15/15 - Step 700] Loss: 0.0146, LR: 0.000000\nEpoch [15/15]\nTrain Loss: 0.0109 | Acc: 99.93% | F1: 1.00\nVal Loss: 0.5361 | Acc: 86.98% | F1: 0.87\nTest Loss: 0.5495 | Acc: 86.09% | F1: 0.86\n\nNumber of trainable parameters: 110592\n\nMemory Breakdown:\nMemory footprint for fine-tuning: 7161.16 MB\nLoRA params contribution: 1.27 MB\n\n==================================================\nRunning experiment with R_LORA = 4\n==================================================\nhead_substring came: heads.head.1.weight\nhead_substring came: heads.head.1.bias\n[Epoch 1/15 - Step 0] Loss: 4.7308, LR: 0.001000\n[Epoch 1/15 - Step 100] Loss: 0.8796, LR: 0.000990\n[Epoch 1/15 - Step 200] Loss: 0.7019, LR: 0.000981\n[Epoch 1/15 - Step 300] Loss: 0.6712, LR: 0.000971\n[Epoch 1/15 - Step 400] Loss: 0.5944, LR: 0.000962\n[Epoch 1/15 - Step 500] Loss: 0.5841, LR: 0.000953\n[Epoch 1/15 - Step 600] Loss: 0.4865, LR: 0.000943\n[Epoch 1/15 - Step 700] Loss: 0.4784, LR: 0.000934\nEpoch [1/15]\nTrain Loss: 0.4079 | Acc: 87.44% | F1: 0.87\nVal Loss: 0.5780 | Acc: 82.42% | F1: 0.82\nTest Loss: 0.5789 | Acc: 81.76% | F1: 0.82\n\n[Epoch 2/15 - Step 0] Loss: 0.4453, LR: 0.000933\n[Epoch 2/15 - Step 100] Loss: 0.3619, LR: 0.000924\n[Epoch 2/15 - Step 200] Loss: 0.4112, LR: 0.000914\n[Epoch 2/15 - Step 300] Loss: 0.4306, LR: 0.000905\n[Epoch 2/15 - Step 400] Loss: 0.4927, LR: 0.000895\n[Epoch 2/15 - Step 500] Loss: 0.5052, LR: 0.000886\n[Epoch 2/15 - Step 600] Loss: 0.4668, LR: 0.000876\n[Epoch 2/15 - Step 700] Loss: 0.4109, LR: 0.000867\nEpoch [2/15]\nTrain Loss: 0.3141 | Acc: 90.31% | F1: 0.90\nVal Loss: 0.5615 | Acc: 83.30% | F1: 0.83\nTest Loss: 0.5533 | Acc: 83.25% | F1: 0.83\n\n[Epoch 3/15 - Step 0] Loss: 0.4002, LR: 0.000867\n[Epoch 3/15 - Step 100] Loss: 0.4216, LR: 0.000857\n[Epoch 3/15 - Step 200] Loss: 0.3361, LR: 0.000848\n[Epoch 3/15 - Step 300] Loss: 0.3846, LR: 0.000838\n[Epoch 3/15 - Step 400] Loss: 0.2750, LR: 0.000829\n[Epoch 3/15 - Step 500] Loss: 0.2816, LR: 0.000819\n[Epoch 3/15 - Step 600] Loss: 0.5503, LR: 0.000810\n[Epoch 3/15 - Step 700] Loss: 0.4801, LR: 0.000800\nEpoch [3/15]\nTrain Loss: 0.2224 | Acc: 92.87% | F1: 0.93\nVal Loss: 0.5111 | Acc: 84.30% | F1: 0.84\nTest Loss: 0.5247 | Acc: 84.33% | F1: 0.84\n\n[Epoch 4/15 - Step 0] Loss: 0.2863, LR: 0.000800\n[Epoch 4/15 - Step 100] Loss: 0.1339, LR: 0.000790\n[Epoch 4/15 - Step 200] Loss: 0.2868, LR: 0.000781\n[Epoch 4/15 - Step 300] Loss: 0.1015, LR: 0.000771\n[Epoch 4/15 - Step 400] Loss: 0.2749, LR: 0.000762\n[Epoch 4/15 - Step 500] Loss: 0.2640, LR: 0.000753\n[Epoch 4/15 - Step 600] Loss: 0.1477, LR: 0.000743\n[Epoch 4/15 - Step 700] Loss: 0.3288, LR: 0.000734\nEpoch [4/15]\nTrain Loss: 0.1727 | Acc: 94.38% | F1: 0.94\nVal Loss: 0.5342 | Acc: 84.54% | F1: 0.84\nTest Loss: 0.5304 | Acc: 84.63% | F1: 0.85\n\n[Epoch 5/15 - Step 0] Loss: 0.2969, LR: 0.000733\n[Epoch 5/15 - Step 100] Loss: 0.1635, LR: 0.000724\n[Epoch 5/15 - Step 200] Loss: 0.2102, LR: 0.000714\n[Epoch 5/15 - Step 300] Loss: 0.2050, LR: 0.000705\n[Epoch 5/15 - Step 400] Loss: 0.1289, LR: 0.000695\n[Epoch 5/15 - Step 500] Loss: 0.2224, LR: 0.000686\n[Epoch 5/15 - Step 600] Loss: 0.2881, LR: 0.000676\n[Epoch 5/15 - Step 700] Loss: 0.1559, LR: 0.000667\nEpoch [5/15]\nTrain Loss: 0.1116 | Acc: 96.55% | F1: 0.97\nVal Loss: 0.4994 | Acc: 85.84% | F1: 0.86\nTest Loss: 0.5108 | Acc: 85.64% | F1: 0.86\n\n[Epoch 6/15 - Step 0] Loss: 0.2294, LR: 0.000667\n[Epoch 6/15 - Step 100] Loss: 0.2160, LR: 0.000657\n[Epoch 6/15 - Step 200] Loss: 0.0514, LR: 0.000648\n[Epoch 6/15 - Step 300] Loss: 0.1326, LR: 0.000638\n[Epoch 6/15 - Step 400] Loss: 0.1307, LR: 0.000629\n[Epoch 6/15 - Step 500] Loss: 0.1324, LR: 0.000619\n[Epoch 6/15 - Step 600] Loss: 0.1201, LR: 0.000610\n[Epoch 6/15 - Step 700] Loss: 0.2149, LR: 0.000600\nEpoch [6/15]\nTrain Loss: 0.0903 | Acc: 97.50% | F1: 0.98\nVal Loss: 0.5442 | Acc: 85.12% | F1: 0.85\nTest Loss: 0.5225 | Acc: 85.09% | F1: 0.85\n\n[Epoch 7/15 - Step 0] Loss: 0.0714, LR: 0.000600\n[Epoch 7/15 - Step 100] Loss: 0.1349, LR: 0.000590\n[Epoch 7/15 - Step 200] Loss: 0.0983, LR: 0.000581\n[Epoch 7/15 - Step 300] Loss: 0.0884, LR: 0.000571\n[Epoch 7/15 - Step 400] Loss: 0.1204, LR: 0.000562\n[Epoch 7/15 - Step 500] Loss: 0.1931, LR: 0.000553\n[Epoch 7/15 - Step 600] Loss: 0.0973, LR: 0.000543\n[Epoch 7/15 - Step 700] Loss: 0.0411, LR: 0.000534\nEpoch [7/15]\nTrain Loss: 0.0556 | Acc: 98.56% | F1: 0.99\nVal Loss: 0.5232 | Acc: 85.90% | F1: 0.86\nTest Loss: 0.5168 | Acc: 86.33% | F1: 0.86\n\n[Epoch 8/15 - Step 0] Loss: 0.0937, LR: 0.000533\n[Epoch 8/15 - Step 100] Loss: 0.0783, LR: 0.000524\n[Epoch 8/15 - Step 200] Loss: 0.0478, LR: 0.000514\n[Epoch 8/15 - Step 300] Loss: 0.0289, LR: 0.000505\n[Epoch 8/15 - Step 400] Loss: 0.0193, LR: 0.000495\n[Epoch 8/15 - Step 500] Loss: 0.1400, LR: 0.000486\n[Epoch 8/15 - Step 600] Loss: 0.0558, LR: 0.000476\n[Epoch 8/15 - Step 700] Loss: 0.0575, LR: 0.000467\nEpoch [8/15]\nTrain Loss: 0.0443 | Acc: 98.85% | F1: 0.99\nVal Loss: 0.5557 | Acc: 85.30% | F1: 0.85\nTest Loss: 0.5609 | Acc: 85.57% | F1: 0.86\n\n[Epoch 9/15 - Step 0] Loss: 0.0106, LR: 0.000467\n[Epoch 9/15 - Step 100] Loss: 0.0472, LR: 0.000457\n[Epoch 9/15 - Step 200] Loss: 0.0243, LR: 0.000448\n[Epoch 9/15 - Step 300] Loss: 0.0238, LR: 0.000438\n[Epoch 9/15 - Step 400] Loss: 0.0424, LR: 0.000429\n[Epoch 9/15 - Step 500] Loss: 0.0324, LR: 0.000419\n[Epoch 9/15 - Step 600] Loss: 0.0760, LR: 0.000410\n[Epoch 9/15 - Step 700] Loss: 0.0226, LR: 0.000400\nEpoch [9/15]\nTrain Loss: 0.0256 | Acc: 99.40% | F1: 0.99\nVal Loss: 0.5469 | Acc: 86.06% | F1: 0.86\nTest Loss: 0.5720 | Acc: 85.84% | F1: 0.86\n\n[Epoch 10/15 - Step 0] Loss: 0.0158, LR: 0.000400\n[Epoch 10/15 - Step 100] Loss: 0.0231, LR: 0.000390\n[Epoch 10/15 - Step 200] Loss: 0.0140, LR: 0.000381\n[Epoch 10/15 - Step 300] Loss: 0.0109, LR: 0.000371\n[Epoch 10/15 - Step 400] Loss: 0.0508, LR: 0.000362\n[Epoch 10/15 - Step 500] Loss: 0.0295, LR: 0.000353\n[Epoch 10/15 - Step 600] Loss: 0.0643, LR: 0.000343\n[Epoch 10/15 - Step 700] Loss: 0.0100, LR: 0.000334\nEpoch [10/15]\nTrain Loss: 0.0163 | Acc: 99.62% | F1: 1.00\nVal Loss: 0.5784 | Acc: 86.24% | F1: 0.86\nTest Loss: 0.5734 | Acc: 86.35% | F1: 0.86\n\n[Epoch 11/15 - Step 0] Loss: 0.0089, LR: 0.000333\n[Epoch 11/15 - Step 100] Loss: 0.0325, LR: 0.000324\n[Epoch 11/15 - Step 200] Loss: 0.0054, LR: 0.000314\n[Epoch 11/15 - Step 300] Loss: 0.0043, LR: 0.000305\n[Epoch 11/15 - Step 400] Loss: 0.0093, LR: 0.000295\n[Epoch 11/15 - Step 500] Loss: 0.0117, LR: 0.000286\n[Epoch 11/15 - Step 600] Loss: 0.0082, LR: 0.000276\n[Epoch 11/15 - Step 700] Loss: 0.0131, LR: 0.000267\nEpoch [11/15]\nTrain Loss: 0.0092 | Acc: 99.88% | F1: 1.00\nVal Loss: 0.5791 | Acc: 86.34% | F1: 0.86\nTest Loss: 0.5900 | Acc: 86.31% | F1: 0.86\n\n[Epoch 12/15 - Step 0] Loss: 0.0050, LR: 0.000267\n[Epoch 12/15 - Step 100] Loss: 0.0199, LR: 0.000257\n[Epoch 12/15 - Step 200] Loss: 0.0030, LR: 0.000248\n[Epoch 12/15 - Step 300] Loss: 0.0022, LR: 0.000238\n[Epoch 12/15 - Step 400] Loss: 0.0057, LR: 0.000229\n[Epoch 12/15 - Step 500] Loss: 0.0022, LR: 0.000219\n[Epoch 12/15 - Step 600] Loss: 0.0039, LR: 0.000210\n[Epoch 12/15 - Step 700] Loss: 0.0234, LR: 0.000200\nEpoch [12/15]\nTrain Loss: 0.0063 | Acc: 99.85% | F1: 1.00\nVal Loss: 0.5921 | Acc: 86.72% | F1: 0.86\nTest Loss: 0.5881 | Acc: 86.90% | F1: 0.87\n\n[Epoch 13/15 - Step 0] Loss: 0.0272, LR: 0.000200\n[Epoch 13/15 - Step 100] Loss: 0.0009, LR: 0.000190\n[Epoch 13/15 - Step 200] Loss: 0.0012, LR: 0.000181\n[Epoch 13/15 - Step 300] Loss: 0.0023, LR: 0.000171\n[Epoch 13/15 - Step 400] Loss: 0.0013, LR: 0.000162\n[Epoch 13/15 - Step 500] Loss: 0.0035, LR: 0.000153\n[Epoch 13/15 - Step 600] Loss: 0.0043, LR: 0.000143\n[Epoch 13/15 - Step 700] Loss: 0.0036, LR: 0.000134\nEpoch [13/15]\nTrain Loss: 0.0023 | Acc: 99.97% | F1: 1.00\nVal Loss: 0.5918 | Acc: 87.12% | F1: 0.87\nTest Loss: 0.5923 | Acc: 87.24% | F1: 0.87\n\n[Epoch 14/15 - Step 0] Loss: 0.0009, LR: 0.000133\n[Epoch 14/15 - Step 100] Loss: 0.0019, LR: 0.000124\n[Epoch 14/15 - Step 200] Loss: 0.0015, LR: 0.000114\n[Epoch 14/15 - Step 300] Loss: 0.0045, LR: 0.000105\n[Epoch 14/15 - Step 400] Loss: 0.0014, LR: 0.000095\n[Epoch 14/15 - Step 500] Loss: 0.0014, LR: 0.000086\n[Epoch 14/15 - Step 600] Loss: 0.0013, LR: 0.000076\n[Epoch 14/15 - Step 700] Loss: 0.0021, LR: 0.000067\nEpoch [14/15]\nTrain Loss: 0.0013 | Acc: 99.98% | F1: 1.00\nVal Loss: 0.5902 | Acc: 86.92% | F1: 0.87\nTest Loss: 0.5951 | Acc: 87.38% | F1: 0.87\n\n[Epoch 15/15 - Step 0] Loss: 0.0011, LR: 0.000067\n[Epoch 15/15 - Step 100] Loss: 0.0015, LR: 0.000057\n[Epoch 15/15 - Step 200] Loss: 0.0011, LR: 0.000048\n[Epoch 15/15 - Step 300] Loss: 0.0007, LR: 0.000038\n[Epoch 15/15 - Step 400] Loss: 0.0010, LR: 0.000029\n[Epoch 15/15 - Step 500] Loss: 0.0010, LR: 0.000019\n[Epoch 15/15 - Step 600] Loss: 0.0006, LR: 0.000010\n[Epoch 15/15 - Step 700] Loss: 0.0009, LR: 0.000000\nEpoch [15/15]\nTrain Loss: 0.0011 | Acc: 99.98% | F1: 1.00\nVal Loss: 0.5934 | Acc: 86.94% | F1: 0.87\nTest Loss: 0.5980 | Acc: 87.34% | F1: 0.87\n\nNumber of trainable parameters: 442368\n\nMemory Breakdown:\nMemory footprint for fine-tuning: 7168.76 MB\nLoRA params contribution: 5.06 MB\n\n==================================================\nRunning experiment with R_LORA = 8\n==================================================\nhead_substring came: heads.head.1.weight\nhead_substring came: heads.head.1.bias\n[Epoch 1/15 - Step 0] Loss: 4.6100, LR: 0.001000\n[Epoch 1/15 - Step 100] Loss: 0.7685, LR: 0.000990\n[Epoch 1/15 - Step 200] Loss: 0.7282, LR: 0.000981\n[Epoch 1/15 - Step 300] Loss: 0.7037, LR: 0.000971\n[Epoch 1/15 - Step 400] Loss: 0.6623, LR: 0.000962\n[Epoch 1/15 - Step 500] Loss: 0.6908, LR: 0.000953\n[Epoch 1/15 - Step 600] Loss: 0.9997, LR: 0.000943\n[Epoch 1/15 - Step 700] Loss: 0.5228, LR: 0.000934\nEpoch [1/15]\nTrain Loss: 0.3880 | Acc: 87.98% | F1: 0.88\nVal Loss: 0.5607 | Acc: 83.32% | F1: 0.83\nTest Loss: 0.5563 | Acc: 83.04% | F1: 0.83\n\n[Epoch 2/15 - Step 0] Loss: 0.4112, LR: 0.000933\n[Epoch 2/15 - Step 100] Loss: 0.3989, LR: 0.000924\n[Epoch 2/15 - Step 200] Loss: 0.1796, LR: 0.000914\n[Epoch 2/15 - Step 300] Loss: 0.2760, LR: 0.000905\n[Epoch 2/15 - Step 400] Loss: 0.2848, LR: 0.000895\n[Epoch 2/15 - Step 500] Loss: 0.3839, LR: 0.000886\n[Epoch 2/15 - Step 600] Loss: 0.5606, LR: 0.000876\n[Epoch 2/15 - Step 700] Loss: 0.2134, LR: 0.000867\nEpoch [2/15]\nTrain Loss: 0.2624 | Acc: 91.54% | F1: 0.91\nVal Loss: 0.5315 | Acc: 83.96% | F1: 0.84\nTest Loss: 0.5407 | Acc: 83.78% | F1: 0.84\n\n[Epoch 3/15 - Step 0] Loss: 0.1726, LR: 0.000867\n[Epoch 3/15 - Step 100] Loss: 0.3005, LR: 0.000857\n[Epoch 3/15 - Step 200] Loss: 0.2365, LR: 0.000848\n[Epoch 3/15 - Step 300] Loss: 0.1812, LR: 0.000838\n[Epoch 3/15 - Step 400] Loss: 0.5268, LR: 0.000829\n[Epoch 3/15 - Step 500] Loss: 0.2440, LR: 0.000819\n[Epoch 3/15 - Step 600] Loss: 0.3278, LR: 0.000810\n[Epoch 3/15 - Step 700] Loss: 0.3443, LR: 0.000800\nEpoch [3/15]\nTrain Loss: 0.1870 | Acc: 94.06% | F1: 0.94\nVal Loss: 0.5313 | Acc: 84.22% | F1: 0.84\nTest Loss: 0.5327 | Acc: 83.56% | F1: 0.84\n\n[Epoch 4/15 - Step 0] Loss: 0.1129, LR: 0.000800\n[Epoch 4/15 - Step 100] Loss: 0.0999, LR: 0.000790\n[Epoch 4/15 - Step 200] Loss: 0.1373, LR: 0.000781\n[Epoch 4/15 - Step 300] Loss: 0.0792, LR: 0.000771\n[Epoch 4/15 - Step 400] Loss: 0.3177, LR: 0.000762\n[Epoch 4/15 - Step 500] Loss: 0.1823, LR: 0.000753\n[Epoch 4/15 - Step 600] Loss: 0.1961, LR: 0.000743\n[Epoch 4/15 - Step 700] Loss: 0.3493, LR: 0.000734\nEpoch [4/15]\nTrain Loss: 0.1356 | Acc: 95.46% | F1: 0.95\nVal Loss: 0.5208 | Acc: 85.14% | F1: 0.85\nTest Loss: 0.5249 | Acc: 84.66% | F1: 0.85\n\n[Epoch 5/15 - Step 0] Loss: 0.1497, LR: 0.000733\n[Epoch 5/15 - Step 100] Loss: 0.1400, LR: 0.000724\n[Epoch 5/15 - Step 200] Loss: 0.0522, LR: 0.000714\n[Epoch 5/15 - Step 300] Loss: 0.1112, LR: 0.000705\n[Epoch 5/15 - Step 400] Loss: 0.1091, LR: 0.000695\n[Epoch 5/15 - Step 500] Loss: 0.0994, LR: 0.000686\n[Epoch 5/15 - Step 600] Loss: 0.2018, LR: 0.000676\n[Epoch 5/15 - Step 700] Loss: 0.1331, LR: 0.000667\nEpoch [5/15]\nTrain Loss: 0.0845 | Acc: 97.50% | F1: 0.98\nVal Loss: 0.5299 | Acc: 85.08% | F1: 0.85\nTest Loss: 0.5421 | Acc: 85.01% | F1: 0.85\n\n[Epoch 6/15 - Step 0] Loss: 0.0521, LR: 0.000667\n[Epoch 6/15 - Step 100] Loss: 0.0684, LR: 0.000657\n[Epoch 6/15 - Step 200] Loss: 0.0537, LR: 0.000648\n[Epoch 6/15 - Step 300] Loss: 0.1409, LR: 0.000638\n[Epoch 6/15 - Step 400] Loss: 0.1121, LR: 0.000629\n[Epoch 6/15 - Step 500] Loss: 0.0797, LR: 0.000619\n[Epoch 6/15 - Step 600] Loss: 0.1874, LR: 0.000610\n[Epoch 6/15 - Step 700] Loss: 0.2825, LR: 0.000600\nEpoch [6/15]\nTrain Loss: 0.0698 | Acc: 97.83% | F1: 0.98\nVal Loss: 0.5736 | Acc: 85.24% | F1: 0.85\nTest Loss: 0.5551 | Acc: 85.23% | F1: 0.85\n\n[Epoch 7/15 - Step 0] Loss: 0.0546, LR: 0.000600\n[Epoch 7/15 - Step 100] Loss: 0.0379, LR: 0.000590\n[Epoch 7/15 - Step 200] Loss: 0.0632, LR: 0.000581\n[Epoch 7/15 - Step 300] Loss: 0.1394, LR: 0.000571\n[Epoch 7/15 - Step 400] Loss: 0.0387, LR: 0.000562\n[Epoch 7/15 - Step 500] Loss: 0.1654, LR: 0.000553\n[Epoch 7/15 - Step 600] Loss: 0.1084, LR: 0.000543\n[Epoch 7/15 - Step 700] Loss: 0.0941, LR: 0.000534\nEpoch [7/15]\nTrain Loss: 0.0394 | Acc: 98.92% | F1: 0.99\nVal Loss: 0.5222 | Acc: 86.04% | F1: 0.86\nTest Loss: 0.5491 | Acc: 85.82% | F1: 0.86\n\n[Epoch 8/15 - Step 0] Loss: 0.0119, LR: 0.000533\n[Epoch 8/15 - Step 100] Loss: 0.0238, LR: 0.000524\n[Epoch 8/15 - Step 200] Loss: 0.0072, LR: 0.000514\n[Epoch 8/15 - Step 300] Loss: 0.0307, LR: 0.000505\n[Epoch 8/15 - Step 400] Loss: 0.0524, LR: 0.000495\n[Epoch 8/15 - Step 500] Loss: 0.0285, LR: 0.000486\n[Epoch 8/15 - Step 600] Loss: 0.0885, LR: 0.000476\n[Epoch 8/15 - Step 700] Loss: 0.0948, LR: 0.000467\nEpoch [8/15]\nTrain Loss: 0.0262 | Acc: 99.32% | F1: 0.99\nVal Loss: 0.5603 | Acc: 86.22% | F1: 0.86\nTest Loss: 0.5604 | Acc: 86.55% | F1: 0.87\n\n[Epoch 9/15 - Step 0] Loss: 0.0168, LR: 0.000467\n[Epoch 9/15 - Step 100] Loss: 0.0115, LR: 0.000457\n[Epoch 9/15 - Step 200] Loss: 0.0137, LR: 0.000448\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-be3a82c009fb>\u001b[0m in \u001b[0;36m<cell line: 297>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"import shutil\n\n# Define the folder to zip and the output zip file name\nfolder_to_zip = '/kaggle/working/logs'\noutput_zip = '/kaggle/working/logs_cifar100_rank1_4.zip'\n\n# Create a zip file\nshutil.make_archive(output_zip.replace('.zip', ''), 'zip', folder_to_zip)\n\nprint(f\"Zipped folder is saved as {output_zip}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:38:57.778152Z","iopub.execute_input":"2025-02-01T09:38:57.778485Z","iopub.status.idle":"2025-02-01T09:38:57.785241Z","shell.execute_reply.started":"2025-02-01T09:38:57.778459Z","shell.execute_reply":"2025-02-01T09:38:57.784418Z"}},"outputs":[{"name":"stdout","text":"Zipped folder is saved as /kaggle/working/logs_cifar100_rank1_4.zip\n","output_type":"stream"}],"execution_count":3}]}